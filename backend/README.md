# NCERT Educational RAG System
## Intel Unnati Project - Multi-Class Textbook Question Answering System

---

## ğŸ“‹ Project Overview

A production-ready Retrieval-Augmented Generation (RAG) system for answering questions from NCERT textbooks across multiple classes (5, 6, 9, 10) with support for English, Hindi, and Urdu languages.

**Key Features:**
- âœ… Multi-class support with isolated vector storage
- âœ… Multi-language support (English, Hindi, Urdu)
- âœ… OCR-based text extraction from PDFs
- âœ… Groq LLM integration with extractive fallback
- âœ… FAISS-based semantic search
- âœ… Subject validation and confidence scoring
- âœ… RESTful API with CORS support

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Frontend (Port 8080)                   â”‚
â”‚                    HTML + CSS + JavaScript                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ HTTP/REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Backend FastAPI (Port 8000)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Pipeline Service (Core)                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚          â”‚          â”‚          â”‚                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   OCR      â”‚ â”‚ Chunking â”‚ â”‚Embedding â”‚ â”‚   Groq LLM  â”‚  â”‚
â”‚  â”‚  Service   â”‚ â”‚ Service  â”‚ â”‚ Service  â”‚ â”‚   Service   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                         â”‚             â”‚         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              FAISS Vector Store                       â”‚  â”‚
â”‚  â”‚  class-5/     class-6/    class-9/    class-10/      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
backend/
â”œâ”€â”€ main.py                      # FastAPI application entry point
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ process_textbooks.py         # Combined processing script
â”œâ”€â”€ test_system.py              # Comprehensive system testing
â”œâ”€â”€ test_llm.py                 # LLM-specific testing
â”œâ”€â”€ verify_chunks.py            # Chunk verification utility
â”‚
â”œâ”€â”€ app/                        # Application core
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes.py          # API endpoints
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ config.py          # Configuration
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py         # Pydantic models
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ ocr_service.py     # PDF OCR processing
â”‚       â”œâ”€â”€ chunking_service.py # Text chunking
â”‚       â”œâ”€â”€ embedding_service.py # Sentence transformers
â”‚       â”œâ”€â”€ faiss_service.py   # Vector search
â”‚       â”œâ”€â”€ groq_service.py    # LLM integration
â”‚       â”œâ”€â”€ pipeline_service.py # Main pipeline
â”‚       â””â”€â”€ safety_service.py  # Validation & safety
â”‚
â”œâ”€â”€ processed_data/             # OCR and cleaned data
â”‚   â””â”€â”€ class-{num}/
â”‚       â””â”€â”€ {subject}/
â”‚           â”œâ”€â”€ ocr_results.json
â”‚           â”œâ”€â”€ cleaned_documents.json
â”‚           â””â”€â”€ chunks.json
â”‚
â”œâ”€â”€ vector_store/               # FAISS indices
â”‚   â”œâ”€â”€ class-5/
â”‚   â”‚   â”œâ”€â”€ english-en/
â”‚   â”‚   â”œâ”€â”€ hindi-hi/
â”‚   â”‚   â””â”€â”€ physical-education-en/
â”‚   â””â”€â”€ class-10/
â”‚       â”œâ”€â”€ all-subjects-english/
â”‚       â””â”€â”€ all-subjects-hindi/
â”‚
â””â”€â”€ logs/                       # Application logs

frontend/
â”œâ”€â”€ index.html                  # Main UI
â”œâ”€â”€ css/
â”‚   â””â”€â”€ styles.css
â””â”€â”€ js/
    â”œâ”€â”€ app.js                  # Main application
    â”œâ”€â”€ api.js                  # API client
    â”œâ”€â”€ ui.js                   # UI components
    â””â”€â”€ config.js               # Frontend config

data/
â”œâ”€â”€ CLASS-V/                    # Class 5 PDFs
â”œâ”€â”€ CLASS-VI/                   # Class 6 PDFs
â”œâ”€â”€ CLASS-IX/                   # Class 9 PDFs
â””â”€â”€ CLASS-X/                    # Class 10 PDFs
```

---

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Navigate to backend
cd "E:\WORK\intel unnati\backend"

# Activate virtual environment
.\.venv\Scripts\Activate.ps1

# Install dependencies (if not already installed)
pip install -r requirements.txt
```

### 2. Start Backend Server

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### 3. Start Frontend Server

```bash
cd "E:\WORK\intel unnati\frontend"
python -m http.server 8080
```

### 4. Access Application

- **Frontend**: http://localhost:8080
- **API Docs**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health

---

## ğŸ“š Processing Textbooks

### Interactive Mode (Recommended)

```bash
python process_textbooks.py
```

This will launch an interactive menu to select:
1. Class number
2. Subject
3. Language

### Command Line Mode

```bash
# Process specific subject
python process_textbooks.py --class 5 --subject English --language en

# Examples
python process_textbooks.py --class 5 --subject Hindi --language hi
python process_textbooks.py --class 10 --subject Science --language en
```

### Processing Pipeline

Each textbook goes through 4 stages:

1. **OCR**: Extract text from PDFs using Tesseract
2. **Cleaning**: Remove noise and normalize text
3. **Chunking**: Split into semantic chunks (400-600 tokens)
4. **Embedding**: Create vectors and FAISS index

**Outputs:**
- `processed_data/class-{num}/{subject}/ocr_results.json`
- `processed_data/class-{num}/{subject}/cleaned_documents.json`
- `processed_data/class-{num}/{subject}/chunks.json`
- `vector_store/class-{num}/{subject}-{lang}/faiss_index.index`

---

## ğŸ§ª Testing

### Comprehensive System Test

Tests all classes, subjects, and languages:

```bash
python test_system.py
```

**Options:**
```bash
# Test specific class
python test_system.py --class 5

# Test with minimal output
python test_system.py --quiet

# Custom API endpoint
python test_system.py --api http://localhost:8000
```

**Test Coverage:**
- Class 5: English, Hindi, Physical Education
- Class 10: English (all subjects), Hindi (all subjects)
- Total: 9 test cases

### LLM Specific Test

Tests Groq API integration:

```bash
python test_llm.py
```

**Options:**
```bash
# Test direct generation only
python test_llm.py --direct

# Test rate limiting
python test_llm.py --rate-limit

# Run all tests
python test_llm.py --all
```

**Test Coverage:**
- API availability check
- Direct generation test
- Rate limiting behavior
- Token usage tracking

---

## ğŸ“Š Current Status

### Processed Classes

| Class | Subject | Language | Status | Chunks | Score Range |
|-------|---------|----------|--------|--------|-------------|
| 5 | English | English | âœ… Complete | 60 | 0.41-0.51 |
| 5 | Hindi | Hindi | âœ… Complete | 91 | 0.45-0.49 |
| 5 | Physical Education | English | âœ… Complete | 39 | N/A |
| 10 | All Subjects | English | âœ… Complete | 623 | 0.42-0.46 |
| 10 | All Subjects | Hindi | âœ… Complete | 371 | 0.46-0.51 |

### Pending Processing

- **Class 5**: Mathematics (16 PDFs), Urdu (20 PDFs)
- **Class 6**: ~50 PDFs across multiple subjects
- **Class 9**: ~71 PDFs across multiple subjects

### Known Issues

1. **Class 5 Physical Education**: Subject validation failing (returns 0.0 score)
   - Index exists with 39 chunks
   - Needs investigation of subject name mapping

2. **Groq Rate Limit**: 100k tokens/day on free tier
   - System automatically falls back to extractive mode
   - Upgrade to Dev tier for unlimited tokens

---

## ğŸ”§ Technical Details

### Models & Libraries

- **Embedding Model**: `sentence-transformers/all-MiniLM-L6-v2` (384D)
- **LLM**: Groq `llama-3.3-70b-versatile`
- **OCR**: Tesseract with PyMuPDF
- **Vector DB**: FAISS with IndexFlatIP
- **Framework**: FastAPI + Pydantic

### Configuration

Key settings in `app/core/config.py`:

```python
# API Configuration
API_URL = "http://localhost:8000"
CORS_ORIGINS = ["http://localhost:8080", "http://localhost:3000"]

# Model Configuration
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
GROQ_MODEL = "llama-3.3-70b-versatile"

# Search Parameters
RELEVANCE_THRESHOLD = 0.45
TOP_K = 30
CHUNK_SIZE = 512
CHUNK_OVERLAP = 50
```

### Class-Specific Validation

Each class has unique filename prefixes for validation:

```python
CLASS_5_PREFIXES = {
    "English": "eesa",
    "Hindi": "ehve",
    "Mathematics": "eemm",
    "Physical Education": "eeky",
    "Urdu": "eust"
}

CLASS_10_PREFIXES = {
    "Health and Physical Education": "jehp",
    "Mathematics": "jemh",
    "Science": "jesc",
    "Social Science": "jess"
}
```

### Special Handling

**Class 10 Structure:**
- Uses combined indices: `all-subjects-english` and `all-subjects-hindi`
- "English" and "Hindi" are treated as language selectors, not subjects
- Subject validation is bypassed for these language-based queries

---

## ğŸ“ API Reference

### POST `/api/query`

Submit a question for answer generation.

**Request Body:**
```json
{
  "question": "What is the importance of reading?",
  "class": 5,
  "subject": "English",
  "language": "en"
}
```

**Response (Success):**
```json
{
  "status": "success",
  "answer": "Reading is important because...",
  "citations": [
    {
      "class": "5",
      "subject": "English",
      "chapter": "Chapter 1",
      "page": "1-16"
    }
  ],
  "grounding_score": 0.9,
  "metadata": {
    "retrieved_chunks": 30,
    "avg_retrieval_score": 0.4523,
    "mode": "groq_llm"
  }
}
```

**Response (Rejected):**
```json
{
  "status": "rejected",
  "reason": "Your question does not appear to be related to English.",
  "rejection_type": "off_topic",
  "metadata": {
    "top_score": 0.3245,
    "threshold": 0.45
  }
}
```

### GET `/health`

Check system health.

**Response:**
```json
{
  "status": "healthy",
  "version": "1.0.0",
  "timestamp": "2026-01-06T12:00:00",
  "components": {
    "faiss_index": "ready",
    "llm": "groq_available"
  }
}
```

---

## ğŸ› ï¸ Troubleshooting

### Backend Not Starting

```bash
# Check if port 8000 is in use
netstat -ano | findstr :8000

# Kill process if needed
taskkill /PID <PID> /F

# Restart backend
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Frontend Connection Issues

1. Check CORS configuration in `app/core/config.py`
2. Ensure backend is running on port 8000
3. Verify frontend is on port 8080

### Low Retrieval Scores

- Check if correct index is loaded for class/subject
- Verify embedding model is initialized
- Review question phrasing (more specific = better results)

### LLM Not Responding

1. Check Groq API key in `.env`
2. Verify rate limit status with `test_llm.py`
3. System will automatically use extractive fallback

---

## ğŸ‘¥ Team & Credits

**Intel Unnati Project Team**

**Technologies Used:**
- FastAPI
- Sentence Transformers
- FAISS
- Groq API
- Tesseract OCR
- PyMuPDF

**Data Source:**
- NCERT Textbooks (Classes 5, 6, 9, 10)

---

## ğŸ“„ License

Educational project for Intel Unnati program.

---

**Last Updated**: January 6, 2026  
**Version**: 1.0.0  
**Status**: Production Ready
